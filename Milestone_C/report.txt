1. Title of project 
	Hidden Markov Model Applications

2. Names and roles of each teammates: 
	Xuedeliang Li 
	Jichun Li

3. What program suppose to do: 
	The program consists of two parts: implementation of Forward Algorithm and 
	Viterbi Algorithm of Hidden Markov Model. The Hidden Markov Model is a 
	statistical Markov model where the system is assumed to be a Markov prcess
	with hidden states. For a given Hidden Markov Model, there are three given
	parameters determine the model: transmission probablity, emission 
	probability and the initial states. For a given sequence, the Forward 
	Algorithm is used to calculate the probablity of a sequence that would be 
	generated from the Hidden Markov Model. The Viterbi Algorithm would produce
	the hidden state sequence that is most likely corresponds to the given 
	observations. 
	We applied two algorithms on two real-life exmaples. Viterbi algorithm is 
	applied on the	finding dishonesty in casino: suppose a gambler owns a set 
	of loaded dice, and loaded dice have a greater opportunity of getting a 
	specific value after they are tossed. During the game, the gambler may or 
	may not use a loaded dice, and therefore we do not know when the gambler is
	cheating. In the case we studied, we assumed that the gambler owns seven 
	dice, one normal dice, and six loaded dice, each loaded dice would have a 
	greater probablity of tossing into a specific side. For example, dice one is
	more likely to get value of 1 after tossing, dice two is more likely to get 
	a value of 2 after tossing, etc. The observable states here is a sequence of
	dice tossing, and what our code would do is based on the given sequence, 
	transmission probability, emission probability and initial selection, 
	finding out which loaded dice is being used. The output would generate a 
	sequence of number representing which dice is being used by the gambler. The
	forward algorithm is applied on relation between weather and bike ride.
	Riding bike is a common choice of commute but the weather is a factor of
	limitation. In our model, we have five types of weather condition: Clear,
	Fog, Fog-Rain, Rain and Rain-Thunderstorm. The weather here is our hidden
	state, and usage of bike is our observable state. Given a sequence of a
	commuter's usage of bike, we can speculate the sequence of weather condition
	outside and obtain the most likely weather on a given day from the sequence.
	

4. Technique used
	There are three major type of questions in Hidden Markov Model: Evaluation
	problem, which asks the probability that a particular sequence of symbols is
	produced by a particular model; Decoding problem, which asks that given a
	sequence of observations, and a model, what is the most likely sequence of
	states that produced the sequence; and Training problem: given a model
	structure and a set of sequences, find the model that best fits the data. In
	this project, what we did is implementing two algorithms of Hidden Markov
	Model: Viterbi Algorithm and Forward Algorithm. Forward Algorithm is mostly
	used in solving evaluation problem and Viterbi Algorithm is mostly used in
	decoding problem. For a Hidden Markov Model, with given transition
	probability, emission probability, initial selection, Viterbi algorithm will
	give the most probable sequence of hidden states, or the Viterbi path that 
	results in a sequence of observed events. The pseudocode of Viterbi
	Algorithm is as below:
	// start of pseudocode
	 function VITERBI
  
    
    {\displaystyle (O,S,\Pi ,Y,A,B):X}
  
{\displaystyle (O,S,\Pi ,Y,A,B):X}
     for each state 
  
    
    {\displaystyle i\in \{1,2,\ldots ,K\}}
  
{\displaystyle i\in \{1,2,\ldots ,K\}} do
         T1[i,1] ← πi·Biy1
         T2[i,1] ← 0
     end for
     for each observation 
  
    
    {\displaystyle i=2,3,\ldots ,T}
  
{\displaystyle i=2,3,\ldots ,T} do
         for each state 
  
    
    {\displaystyle j\in \{1,2,\ldots ,K\}}
  
{\displaystyle j\in \{1,2,\ldots ,K\}} do
             {\displaystyle T_{1}[j,i]\gets \max _{k}{(T_{1}[k,i-1]\cdot
A_{kj}\cdot B_{jy_{i}})}} {\displaystyle T_{1}[j,i]\gets \max
_{k}{(T_{1}[k,i-1]\cdot A_{kj}\cdot B_{jy_{i}})}}
             {\displaystyle T_{2}[j,i]\gets \arg \max _{k}{(T_{1}[k,i-1]\cdot
A_{kj}\cdot B_{jy_{i}})}} T_{2}[j,i]\gets \arg \max _{k}{(T_{1}[k,i-1]\cdot
A_{kj}\cdot B_{jy_{i}})}
         end for
     end for
     {\displaystyle z_{T}\gets \arg \max _{k}{(T_{1}[k,T])}} z_{T}\gets \arg
\max _{k}{(T_{1}[k,T])}
     xT ← szT
     for i ← T,T-1,...,2 do
         zi-1 ← T2[zi,i]
         xi-1 ← szi-1
     end for
     return X
 end function
	// end of pseudocode
	The Forward Algorithm will give the probability of each hidden state for a
	given Hidden Markov Model. The code of Forward Algorithm is as below
	// start of pseudocode
	init {\displaystyle t=0} t=0, transition probabilities {\displaystyle
p(x_{t}|x_{t-1})} p(x_{t}|x_{{t-1}}), emission probabilities, {\displaystyle
p(y_{j}|x_{i})} {\displaystyle p(y_{j}|x_{i})}, observed sequence,
{\displaystyle y(1:t)} {\displaystyle y(1:t)}

       for 
  
    
    {\displaystyle t=t+1}
  
t=t+1
           
  
    
    {\displaystyle \alpha _{t}(x_{t})=p(y_{t}|x_{t})\sum
_{x_{t-1}}p(x_{t}|x_{t-1})\alpha _{t-1}(x_{t-1})}
  
\alpha _{t}(x_{t})=p(y_{t}|x_{t})\sum _{{x_{{t-1}}}}p(x_{t}|x_{{t-1}})\alpha
_{{t-1}}(x_{{t-1}}).
       until t=T
return {\displaystyle p(y(1:t))=\alpha _{T}} {\displaystyle p(y(1:t))=\alpha
_{T}}
	// end of pseudocode
	Due to the nature of the Forward Algorithm and Viterbi Algorithm, the value
	calculated previously are used in later calculation repeatedly, so the
	actual implementation of algorithm utilized dynamic programming technique. 
	





